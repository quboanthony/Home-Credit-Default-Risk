{
  "cells": [
    {
      "metadata": {
        "_uuid": "bbc03d416cd4a5af7ef8222875b3b2cde288f823"
      },
      "cell_type": "markdown",
      "source": "# Manual Feature Engineering"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b45ddfc692b03c055082c9bf8d97807be7022c9"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78aa0ad60faa4aee2c7c994a150591d6ec911fb1"
      },
      "cell_type": "code",
      "source": "bureau=pd.read_csv('../input/bureau.csv')\nbureau.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c0a76e573ce3cdd1edb347eac6a9a2a301e2c65"
      },
      "cell_type": "code",
      "source": "previous_loan_counts=bureau.groupby('SK_ID_CURR',as_index=False)['SK_ID_BUREAU'].count().rename(columns={'SK_ID_BUREAU':'previous_loan_counts'})\n\nprevious_loan_counts.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86e5e6f5d228d5a33492c0793243160c4307d50e"
      },
      "cell_type": "code",
      "source": "train=pd.read_csv('../input/application_train.csv')\ntrain=train.merge(previous_loan_counts,on='SK_ID_CURR',how='left')\n\ntrain['previous_loan_counts']=train['previous_loan_counts'].fillna(0)\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bdc62730d270ffed7b1574510b6b9d4733a90282"
      },
      "cell_type": "markdown",
      "source": "# Assessing Usefulness of New Variabel with r value\n\n- Pearson Correlation Coefficient(r-value) between this variable and the target."
    },
    {
      "metadata": {
        "_uuid": "a1a26822564a9d7bab623aa4d40a25c710cbf44d"
      },
      "cell_type": "markdown",
      "source": "## Kernel Density Estimate Plots"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72fd933f98c9687dfc279289b600ae7ece0a4568"
      },
      "cell_type": "code",
      "source": "def kde_target(var_name,df):\n    \n    corr=df['TARGET'].corr(df[var_name])\n    \n    avg_repaid=df.ix[df['TARGET']==0,var_name].median()\n    avg_not_repaid=df.ix[df['TARGET']==1,var_name].median()\n    \n    plt.figure(figsize=(12,6))\n    \n    sns.kdeplot(df.ix[df['TARGET']==0,var_name],label='target==0')\n    sns.kdeplot(df.ix[df['TARGET']==1,var_name],label='target==1')\n    \n    plt.title('Density of %s by Target Value'%var_name)\n    plt.xlabel('%s'%var_name)\n    plt.ylabel('Density')\n    plt.legend()\n    \n    print('The correlation between %s and the Target is %0.4f'%(var_name,corr))\n    print('Median value of %s for repaid= %0.4f'%(var_name,avg_repaid))\n    print('Median value of %s for not repaid= %0.4f'%(var_name,avg_not_repaid))\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18a0cc98a7b58c90dcaf23383213d5c9ac30e42a"
      },
      "cell_type": "code",
      "source": "kde_target('EXT_SOURCE_3',train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "756c3fe42c27dcb57a18e807ce02084b814fb1b9"
      },
      "cell_type": "code",
      "source": "kde_target('previous_loan_counts',train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "43477c967f414cd31790dc7520eacbf8f0849d98"
      },
      "cell_type": "markdown",
      "source": "# Aggregating numerical "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8318e6fbda9e633f81aa560944f0d1a4f0fccafe"
      },
      "cell_type": "code",
      "source": "bureau_agg=bureau.drop(columns=['SK_ID_BUREAU']).groupby('SK_ID_CURR',as_index=False).agg(['count','mean','max','min','sum']).reset_index()\nbureau_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a644a57124802924c45f263ef8970c288266654"
      },
      "cell_type": "code",
      "source": "columns=['SK_ID_CURR']\n\nfor var in bureau_agg.columns.levels[0]:\n    if var !='SK_ID_CURR':\n        for stat in bureau_agg.columns.levels[1][:-1]:\n            columns.append('bureau_%s_%s'%(var,stat))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ad40eb96e5fb36c1bd9f63941c7649b0d664f72"
      },
      "cell_type": "code",
      "source": "bureau_agg.columns=columns\nbureau_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db721b34e65e09624305103bf710fdc02c965ca2"
      },
      "cell_type": "code",
      "source": "train=train.merge(bureau_agg,on='SK_ID_CURR',how='left')\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fc0f97d6909c6a0843235ee4d80798d5d5fce1b1"
      },
      "cell_type": "markdown",
      "source": "## Correlation of Aggregated Values with Target"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d0787bafa247fe2b222d0fc8a78cc053670ef842"
      },
      "cell_type": "code",
      "source": "new_corrs=[]\n\nfor col in columns:\n    corr=train['TARGET'].corr(train[col])\n    \n    new_corrs.append((col,corr))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88a01341abfaedf1c657c95667f4265c4aa214d4"
      },
      "cell_type": "code",
      "source": "new_corrs=sorted(new_corrs,key=lambda x: abs(x[1]),reverse=True)\nnew_corrs[:15]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f052f5ebe754c1d1cd87fd9f2e3e03a221533e1"
      },
      "cell_type": "code",
      "source": "kde_target('bureau_DAYS_CREDIT_mean',train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "60255893510b8579dbc6856bb68dca6618e652fb"
      },
      "cell_type": "markdown",
      "source": "# Function for Numerica Aggregations"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81131aac1aa59ac9ced257db88dc0f03e67d98aa"
      },
      "cell_type": "code",
      "source": "def agg_numeric(df,group_var,df_name):\n    for col in df:\n        if col!=group_var and 'SK_ID' in col:\n            df=df.drop(columns=col)\n    \n    group_ids=df[group_var]\n    numeric_df=df.select_dtypes('number')\n    numeric_df[group_var]=group_ids\n    \n    agg=numeric_df.groupby(group_var).agg(['count','mean','max','min','sum']).reset_index()\n    \n    columns=[group_var]\n    \n    for var in agg.columns.levels[0]:\n        if var!=group_var:\n            for stat in agg.columns.levels[1][:-1]:\n                columns.append('%s_%s_%s'%(df_name,var,stat))\n    agg.columns=columns\n    return agg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "441b9ce10ec3cb5aace9df7b3613277d1925f7e4"
      },
      "cell_type": "code",
      "source": "bureau_agg_new=agg_numeric(bureau.drop(columns=['SK_ID_BUREAU']),group_var='SK_ID_CURR',df_name='bureau')\nbureau_agg_new.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1dbaba77f8c6a9d7fa69973c8053e3bfc025f41"
      },
      "cell_type": "code",
      "source": "bureau_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11e3825f6552571c6f583e4fd430bcac4b54f1a5"
      },
      "cell_type": "code",
      "source": "def target_corrs(df):\n    \n    corrs=[]\n    \n    for col in df.columns:\n        print(col)\n        if col != 'TARGET':\n            corr=df['TARGET'].corr(df[col])\n            \n            corrs.append(corr)\n            \n    corrs=sorted(corrs,key=lambda x: abs(x[1]),reverse=True)\n    \n    return corrs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3a5b1345cf0955e22b71f3eeff646783dc22c276"
      },
      "cell_type": "markdown",
      "source": "# Categorical Variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31ab2cc22881b1facd347b09b4934b5c31520747"
      },
      "cell_type": "code",
      "source": "categorical=pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR']=bureau['SK_ID_CURR']\ncategorical.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "1226b7cf0244564ca4048dc06846fcc8bae1bd3b"
      },
      "cell_type": "code",
      "source": "categorical_grouped=categorical.groupby('SK_ID_CURR').agg(['sum','mean'])\ncategorical_grouped.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de067f2f34307322e42ea69ff278b427630cac4a"
      },
      "cell_type": "code",
      "source": "categorical_grouped.columns.levels[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f1e16c9930c07544e5223db7292ce2d9b287eaa"
      },
      "cell_type": "code",
      "source": "categorical_grouped.columns.levels[1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e1f8ded8176bf25d985967b3d5166799a88929d"
      },
      "cell_type": "code",
      "source": "group_var='SK_ID_CURR'\n\ncolumns=[]\n\nfor var in categorical_grouped.columns.levels[0]:\n    if var!=group_var:\n        for stat in ['count','count_sum']:\n            columns.append('%s_%s'%(var,stat))\n            \ncategorical_grouped.columns=columns\n\ncategorical_grouped.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf6eceb9a5748154c16bae95e27a4adab8fd58db"
      },
      "cell_type": "code",
      "source": "train=train.merge(categorical_grouped,on='SK_ID_CURR',how='left')\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93742bd5573ab347eadffe0ee0542484bab0490f"
      },
      "cell_type": "code",
      "source": "train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14a0d3f548aa7aaa243e4abdeaee184e5573f911"
      },
      "cell_type": "code",
      "source": "train.iloc[:10,123:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e84bfa9d6e6e74a767c971a133431d21ac80ee83"
      },
      "cell_type": "markdown",
      "source": "## Function to Handle Categorical Variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3cc04b70e16f182fb09ff7b607d22d1a950502d"
      },
      "cell_type": "code",
      "source": "def count_categorical(df,group_var,df_name):\n    categorical=pd.get_dummies(df.select_dtypes('object'))\n    categorical[group_var]=df[group_var]\n    \n    categorical=categorical.groupby(group_var).agg(['sum','mean'])\n    \n    columns=[]\n    \n    for var in categorical.columns.levels[0]:\n        if var!=group_var:\n            for stat in ['count','count_norm']:\n                columns.append('%s_%s_%s'%(df_name,var,stat))\n    \n    categorical.columns=columns\n    \n    return categorical",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d30450017b2fc07a97df8e56e6007a0b1b811135"
      },
      "cell_type": "code",
      "source": "bureau_counts=count_categorical(bureau,'SK_ID_CURR','bureau')\nbureau_counts.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8c7e85c73a0bcb64ad24eb57d11a41b89c31d19"
      },
      "cell_type": "markdown",
      "source": "## Applying Operations to another dataframe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10497a93478789f6a332b8eed9a0cf95869064c6"
      },
      "cell_type": "code",
      "source": "bureau_balance=pd.read_csv('../input/bureau_balance.csv')\nbureau_balance.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf104cca2ce51a532cfa92ccddd9077d310261dc"
      },
      "cell_type": "code",
      "source": "bureau_balance_counts=count_categorical(bureau_balance,group_var='SK_ID_BUREAU',df_name='bureau_balance')\nbureau_balance_counts.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f962b4fcdc38e67c61e96d73a7cc9bc4da2e072"
      },
      "cell_type": "code",
      "source": "bureau_balance_agg=agg_numeric(bureau_balance,group_var='SK_ID_BUREAU',df_name='bureau_balance')\nbureau_balance_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c9ab9b1170d0244809f3a1f25de57fd961f8dee"
      },
      "cell_type": "code",
      "source": "bureau_by_loan=bureau_balance_agg.merge(bureau_balance_counts,right_index=True, left_on='SK_ID_BUREAU',how='outer')\n\nbureau_by_loan=bureau_by_loan.merge(bureau[['SK_ID_BUREAU','SK_ID_CURR']],on='SK_ID_BUREAU',how='left')\n\nbureau_by_loan.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "484ee8990c8c78e796e2d5676f4d95107c324c94"
      },
      "cell_type": "code",
      "source": "bureau_balance_by_client=agg_numeric(bureau_by_loan.drop(columns=['SK_ID_BUREAU']),group_var='SK_ID_CURR',df_name='client')\n\nbureau_balance_by_client.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4674a8ef9aff6967e445fb6fa280ddce4f11d43f"
      },
      "cell_type": "markdown",
      "source": "In bureau_balance dataframe:\n1. Calculate numeric stats grouping by each loan\n2. Made value counts of each categorical variable grouping by loan\n3. Merge the stats and value counts on the loans\n4. Calculated numeric stats for the resulting dataframe grouping by the client id\n\nFinal resulting dataframe has one row for each client with statistics calculated for all of their loans with monthly balance information"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d28116b6d100fb433e391ceb0f6cebfdff13961e"
      },
      "cell_type": "markdown",
      "source": "## Put functions together\n\ndelect the former variables and recalculate these variables from the groudup"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5fa6d9ef57320b66826c5ba2c46d4deb0b1d703"
      },
      "cell_type": "code",
      "source": "import gc\ngc.enable()\ndel train,bureau,bureau_balance,bureau_agg,bureau_agg_new,bureau_balance_agg,bureau_balance_counts, bureau_by_loan,bureau_balance_by_client,bureau_counts\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ab53d6260a7fcd99a54633481c706e1b9fe98ce",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train=pd.read_csv('../input/application_train.csv')\nbureau=pd.read_csv('../input/bureau.csv')\nbureau_balance=pd.read_csv('../input/bureau_balance.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f71297fba00ae5866157215b3f0dc5d23a9c4823"
      },
      "cell_type": "markdown",
      "source": "## Counts of Bureau Dataframe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a97972aed04527be044f1f34234c45518f24cce"
      },
      "cell_type": "code",
      "source": "bureau_counts=count_categorical(bureau,group_var='SK_ID_CURR',df_name='bureau')\nbureau_counts.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "143b880e3561d8765449655e2366ff24fd9e4261"
      },
      "cell_type": "markdown",
      "source": "## Aggregated Stats of Bureau Dataframe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "927ab63734904022a938f99b52d06c99d4a66a1b"
      },
      "cell_type": "code",
      "source": "bureau_agg=agg_numeric(bureau.drop(columns='SK_ID_BUREAU'),group_var='SK_ID_CURR',df_name='bureau')\nbureau_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "db10d82010d0bf41352b9fd3ef4979e0eef3350b"
      },
      "cell_type": "markdown",
      "source": "## Value counts of Bureau Balance dataframe by loan"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f47812a6bdb614c8e9f012e9fa566b4b361be621"
      },
      "cell_type": "code",
      "source": "bureau_balance_counts=count_categorical(bureau_balance,group_var='SK_ID_BUREAU',df_name='bureau_balance')\nbureau_balance_counts.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3224366faef500b0f25caa8a2b206d87db519f3d"
      },
      "cell_type": "markdown",
      "source": "## Aggregated stats of Bureau Balance dataframe by loan"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84763e4669b02eac15c8511051febfad274f7870"
      },
      "cell_type": "code",
      "source": "bureau_balance_agg=agg_numeric(bureau_balance, group_var='SK_ID_BUREAU', df_name='bureau_balance')\nbureau_balance_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b468c583e31876b188df39a86d12385695b6e08"
      },
      "cell_type": "markdown",
      "source": "## Aggregated Stats of Bureau Balance by Client"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e53dbe1ba496f5d202342d135016014dd7415322"
      },
      "cell_type": "code",
      "source": "bureau_by_loan=bureau_balance_agg.merge(bureau_balance_counts, right_index=True,left_on='SK_ID_BUREAU',how='outer')\nbureau_by_loan=bureau[['SK_ID_BUREAU','SK_ID_CURR']].merge(bureau_by_loan,on='SK_ID_BUREAU',how='left')\n\nbureau_balance_by_client=agg_numeric(bureau_by_loan.drop(columns=['SK_ID_BUREAU']),group_var='SK_ID_CURR',df_name='client')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eab5c7a4cf67ee88883b2c780e8c2784b38209c5"
      },
      "cell_type": "markdown",
      "source": "## Insert Computed Features into Training Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd087a388eb1b2a3dfebddb13bca3187e48a2c2a"
      },
      "cell_type": "code",
      "source": "original_features=list(train.columns)\nprint('Original Number of Features: ', len(original_features))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc7390b612b570d557b7ba575d6d27be52b0d554"
      },
      "cell_type": "code",
      "source": "#Merge with value counts of bureau\ntrain=train.merge(bureau_counts,on='SK_ID_CURR',how='left')\n\n#Merge with the stats of bureau\ntrain=train.merge(bureau_agg,on='SK_ID_CURR',how='left')\n\n#Merge with the monthly information grouped by client\ntrain=train.merge(bureau_balance_by_client, on='SK_ID_CURR',how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7eb6a5b9a5fbb7c0d756f56452c98bfb1c2498dc"
      },
      "cell_type": "code",
      "source": "new_features=list(train.columns)\nprint('Number of features using previous loans from other institutions data: ', len(new_features))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55ad3413e6658edeacc534b9bc96445468b42bd4"
      },
      "cell_type": "markdown",
      "source": "# Feature Engineering Outcomes\n\nAfter all that work, now we want to take a look at the variables we have created. We can look at the percentage of missing values, the correlations of variables with the target, and also the correlation of variables with the other variabels. The correlations between variables can show if we have colinear variables, that is , variables that are highly correlated with one another. Often, we want to remove one in a pair of colinear variables because having both variables would be redundant. We can also use the percentage of missing values to remove features with a substantial majority of values that are not present.\n\n**Feature selection** will be an important focus going forward, because reducing the number of features can help the model learn during training and also generalize better to the testing data. The \"curse of dimensionality'' is the name given to the issues caused by having too many features (too high of a dimension). As the number of variables increases, the number of datapoints needed to learn the relationship between these variables and the target value increase exponentially.\n\nFeature selection is the process of removing variables to help our model to learn and generalize better to the testing set. The objective is to remove useless/redundant variables while preserving those that useful. These are a number of tools we can use for this process, but in this notebook we will stick to removing columns with a high percentage of missing values and variables that have a high correlation with one another. Later we can look at using the features importances returned from models such as the Gradient Boosting Machine or Random Forest to perform feature selection."
    },
    {
      "metadata": {
        "_uuid": "cef73c12a420811597044f1d86515e36de6e429b"
      },
      "cell_type": "markdown",
      "source": "## Missing Values\nAn important consideration is the missing values in the dataframe. Columns with too many missing values might have to be dropped."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7e5060b8f4a198bc8ff8a62d7017fb80efdf767"
      },
      "cell_type": "code",
      "source": "# Function to caluculate missing values by column\ndef missing_values_table(df):\n    \n    #Total missing values\n    mis_val=df.isnull().sum()\n    \n    #Percentage of missing values\n    mis_val_percent=100*df.isnull().sum()/len(df)\n    \n    #Make a table with the results\n    mis_val_table=pd.concat([mis_val,mis_val_percent],axis=1)\n    \n    #Rename the columns\n    mis_val_table_ren_columns=mis_val_table.rename(columns={0:'Missing Values',1:'% of Total Values'})\n    \n    #Sort th table by percentage of missing descending \n    mis_val_table_ren_columns=mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values('% of Total Values', ascending=False).round(1)\n    \n    # print some summary information\n    print('Your selected dataframe has '+str(df.shape[1])+ \" columns.\\n\" \"There are \"+str(mis_val_table_ren_columns.shape[0])+' columns that have missing values.')\n    \n    return mis_val_table_ren_columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32824b87b42861e341eadf0cdc1f6fc12b908a36"
      },
      "cell_type": "code",
      "source": "missing_train=missing_values_table(train)\nmissing_train.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9c76e34df3f7f31d2f919f663a33df5b1fc68d49"
      },
      "cell_type": "markdown",
      "source": "We see that there are number of columns with a high percentage of missing values. There is no well-established threshold for removing missing values, and the best course of action depends on the problem. Here, to reduce the number of features, we will remove any columns in either the training or the testing data that have greater than 90% missing values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fb3df25d4066fabecf6e50333e0a6c5d20cdd06"
      },
      "cell_type": "code",
      "source": "missing_train_vars=list(missing_train.index[missing_train['% of Total Values']>90.0])\nlen(missing_train_vars)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "05f28cc88727914bfa8ea9bb81152d05b1211d3d"
      },
      "cell_type": "markdown",
      "source": "Before we remove the missing values, we will find the missing value percentages in the testing data. We will then remove any columns with greater than 90% missing values in either the training or testing data. Let's now read in the testing data, perform the same operations, and look at the missing values in the testing data. We already have caluculated all the counts and aggregation statistics, so we only need to merge the testing data with the appropriate data."
    },
    {
      "metadata": {
        "_uuid": "4fb846de6d62afaea6f2907b66655a5b60139deb"
      },
      "cell_type": "markdown",
      "source": "## Calculating information for Testing Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e6b1b448a4349612ce9b22d453c10e7d86d6a79"
      },
      "cell_type": "code",
      "source": "#Reading the test dataframe\ntest=pd.read_csv('../input/application_test.csv')\n\n#Merge with the stats of bureau\ntest=test.merge(bureau_counts,on='SK_ID_CURR',how='left')\n\n#Merge with the value counts of bureau balance\ntest=test.merge(bureau_agg,on='SK_ID_CURR',how='left')\n\n#Merge with the balue counts of bureau balance\ntest=test.merge(bureau_balance_by_client, on='SK_ID_CURR',how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a33c77f7a419981042709f5c7fea76ac059890f"
      },
      "cell_type": "code",
      "source": "print('Shape of Testing Data: ',test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "badf6f1430edc0dc4cdf7e64d814f1f74a880496"
      },
      "cell_type": "markdown",
      "source": "We need  to align the testing and training dataframes, which means matching up the columns so they have the exact  same columns, This shouldn't be an issue here, but when we one-hot encode variables, we need to align the dataframes to make sure they have the same columns."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9faa86c97a53f569a3156f123602ce832dc65305"
      },
      "cell_type": "code",
      "source": "missing_test=missing_values_table(test)\nmissing_test.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b30a9d30d2827647d3e70113676194782ef3796"
      },
      "cell_type": "code",
      "source": "missing_test_vars=list(missing_test.index[missing_test['% of Total Values']>90])\nlen(missing_test_vars)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18a2ccf9e24c1ea0edc8fbafcda258af66236482"
      },
      "cell_type": "code",
      "source": "missing_columns=list(set(missing_test_vars+missing_train_vars))\nlen(missing_columns)\nprint('There are %d columns with more than 90%% missing in either the training or testing data.'%len(missing_columns))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f7f2bf765997d0cd7a1c847a0af794892ee7975"
      },
      "cell_type": "code",
      "source": "train=train.drop(columns=missing_columns)\ntest=test.drop(columns=missing_columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd9bec6451dc3a81319dd5f7ffd5befaf7c72d25"
      },
      "cell_type": "markdown",
      "source": "We ended up removing no  columns in this round because there are no columns with more than 90% missing values. We might have to apply another feature selection method to reduce the dimensionality."
    },
    {
      "metadata": {
        "_uuid": "3d5f2e682c7abe8f66aa84f18b8d2a5e0924b139"
      },
      "cell_type": "markdown",
      "source": "At this point we will save both the training and testing data. I encourage anyone to try different percentages for dropping the missing columns and compare the outcomes."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dbb056a86c63ffc26e1513921f58be8f109d532"
      },
      "cell_type": "code",
      "source": "train.to_csv('train_bureau_raw.csv',index=False)\ntest.to_csv('test_bureau_raw.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c447b066c88ea55a5a8a85613dae8ab95ef14f0"
      },
      "cell_type": "markdown",
      "source": "## Correlations\nFirst let's look at the correlations of the variables with the target. We can see in any of the varaibles we created have a greater correlation than those already present in the training data(from application)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed88a053b377ae259fafda32a844ef9f2bd4a2fb"
      },
      "cell_type": "code",
      "source": "# Calculate all correlations in dataframe\ncorrs=train.corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb3ebe54a9f14ec27c8b9b91aeafa80549245978"
      },
      "cell_type": "code",
      "source": "corrs=corrs.sort_values('TARGET',ascending=False)\n\n#Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ef8b50ead0ef2f82304fab31f9ca18b057e1861"
      },
      "cell_type": "code",
      "source": "pd.DataFrame(corrs['TARGET'].dropna().tail(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "02ec5e21e7b47a95c24f7a4fc1c78c420eea43a9"
      },
      "cell_type": "markdown",
      "source": "The highest correlated variable with the target (other than the TARGET which of course has a correlation of 1), is a variable we created. However, just because the variable is correlated does not mean that ti will be useful, and we have to remember that **if we  generate hundreds of new variables, some are going to be correlated with the target simpy because of random noise.**\n\nViewing the correlations skeptically, it does appear that several of the newly created variables may be useful. **To assess the 'usefulness' of variables, we will look at the feature importances return by the model.** For curiousity's sake (and because we already wrote the function) we can make a kde plot of two of the newly created variables."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7783bd519468a88351582850f459c5874e12c411"
      },
      "cell_type": "code",
      "source": "kde_target(var_name='client_bureau_balance_MONTHS_BALANCE_min_mean',df=train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e639287c1af5a5d3b26886e635b26d97ac6823b5"
      },
      "cell_type": "code",
      "source": "kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm',df=train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "518459bfb186194268aeaf5cc936592c6a84dcb3"
      },
      "cell_type": "markdown",
      "source": "Well this last distribution is all over the place. This variable represents the number of previous loans with a CREDIT_ACTIVE value of Active divided by the total number of previous loans for a client. The correlation here is so weak that I do not think we should draw any conclusions!"
    },
    {
      "metadata": {
        "_uuid": "7b3b45f12040f10976b3c7523b887f95b9af926e"
      },
      "cell_type": "markdown",
      "source": "### Colinear Variables\nWe can calculate not only the correlations of the variables with the target, but also the correlation of each variable with every other variable. This will allow us to see if there are highly colinear variables that should perhaps be removed from the data.\n\nLet's look for any variables that have a greater than 0.8 correlation with other variables."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9580b0cd968113d18c7283971d25fde9a1a49064"
      },
      "cell_type": "code",
      "source": "# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars={}\n\n#For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col]=list(corrs.index[corrs[col]>threshold])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "48780d8606cd94795001707fbfa552d6ac231536"
      },
      "cell_type": "markdown",
      "source": "For each of these pairs of highly corrlated variables, we only want to remove one of the variables. The following code creates a set of variables to remove by only adding one of each pair."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a379ff70e179ac1f48fb37bb805e8de322a85d2b"
      },
      "cell_type": "code",
      "source": "\n# Track columns to remove  and columns aleardy examined\ncols_to_remove=[]\ncols_seen=[]\ncols_to_remove_pair=[]\n# Iterate through columns and correlated columns\nfor key,value in above_threshold_vars.items():\n    # Keep track of the columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            #only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n                \ncols_to_remove=list(set(cols_to_remove))\nprint('Number of columns to remove: ',len(cols_to_remove))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "817506c430f44ee81c8a63bd56f74960a28a9b8e"
      },
      "cell_type": "markdown",
      "source": "We can remove these columns from both the training and the testing datasets. We will have to compare performance after removing these variables with performance keeping these variables (the raw csv files we saved earlier)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0365b074081b20ea82d375c22b99a34d51e178cd"
      },
      "cell_type": "code",
      "source": "train_corrs_removed=train.drop(columns=cols_to_remove)\ntest_corrs_removed=test.drop(columns=cols_to_remove)\n\nprint('Training Corrs Removed Shape: ',train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ',test_corrs_removed.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6f4deb62a41abe82c76916b45deb0f99e40f900"
      },
      "cell_type": "code",
      "source": "train_corrs_removed.to_csv('train_bureau_corrs_removed.csv',index=False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv',index=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb41128c56b3c58125822e3f7b91c3cd3a5fda44"
      },
      "cell_type": "markdown",
      "source": "# Modeling\nTo actually test the performance of these new datasets, we will try using them for machine learning! Here we will use a function I developed in another notebook to compare the features (the raw version with the highly correlated variables removed). We can run this kind of like an experiment, and the contril will be the performance of just the application data in this function when submitted to the competition. I've already recorded that performance, so we can list out our control and our two test conditions:\n\n**For all datasets, use the model shown below (with the exact hyperparameters)**\n\n- control: only the data in the application files.\n- test one: the data in applcation files with all of the data recorded from the bureau and bureau_balance files\n- test two: the data in the application files with all of the data recordedd from the bureau and bureau_balance files with highly correlated varaibles removed."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2403198570f5a084b9da1bf1f3175fc520f27923"
      },
      "cell_type": "code",
      "source": "import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a52fd46ed482ad4ce5f1fe3595a19e011247bc4"
      },
      "cell_type": "code",
      "source": "def model(features, test_features, encoding='ohe', n_folds=5):\n    \"\"\"\n    Train and test a light gradient boosting model using cross validation.\n    \n\n    \n    \"\"\"\n    \n    train_ids=features['SK_ID_CURR']\n    test_ids=test_features['SK_ID_CURR']\n    \n    labels=features['TARGET']\n    \n    features=features.drop(columns=['SK_ID_CURR','TARGET'])\n    test_features=test_features.drop(columns=['SK_ID_CURR'])\n    \n    \n    if encoding=='ohe':\n        features=pd.get_dummies(features)\n        test_features=pd.get_dummies(test_features)\n        \n        features, test_features=features.align(test_features,join='inner',axis=1)\n        \n        cat_indices='auto'\n        \n    elif encoding=='le':\n        \n        label_encoder=labelEncoder()\n        \n        cat_indices=[]\n        \n        for i,col in enumerate(features):\n            if features[col].dtype=='object':\n                features[col]=label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col]=label_encoder.fit_transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n                \n                cat_indices.append(i)\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n    \n    print('Training Data shape: ',features.shape)\n    print('Testing Data shape: ',test_features.shape)\n    \n    feature_names=list(features.columns)\n    \n    features=np.array(features)\n    test_features=np.array(test_features)\n    \n    #Create the kfold object\n    k_fold=KFold(n_splits=n_folds,shuffle=False,random_state=50)\n    \n    #Empty array for feature importances\n    feature_importance_values=np.zeros(len(feature_names))\n    \n    #Empty array for test predictions\n    test_predictions=np.zeros(test_features.shape[0])\n    \n    #Empty array for out of fold validation predictions\n    out_of_fold=np.zeros(features.shape[0])\n    \n    #Lists for recording validation and training scores\n    valid_scores=[]\n    train_scores=[]\n    \n    #Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        #Training data for the fold\n        train_features,train_labels=features[train_indices],labels[train_indices]\n        \n        #Validation data for the fold\n        valid_features,valid_labels=features[valid_indices],labels[valid_indices]\n        \n        #Create the model\n        model=lgb.LGBMClassifier(n_estimators=10000,objective='binary', class_weight='balanced', learning_rate=0.05, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8,n_jobs=-1,random_state=50)\n        \n        #Train the model\n        model.fit(train_features, train_labels, eval_metric='auc',eval_set=[(valid_features, valid_labels),(train_features,train_labels)], eval_names=['valid','train'],categorical_feature=cat_indices, early_stopping_rounds=100, verbose=200)\n        \n        # Record the best iteration\n        best_iteration=model.best_iteration_\n        \n        #Record the feature importances\n        feature_importance_values+=model.feature_importances_/k_fold.n_splits\n        \n        #Make predictions\n        test_predictions+=model.predict_proba(test_features,num_iteration=best_iteration)[:,1]/k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices]=model.predict_proba(valid_features,num_iteration=best_iteration)[:,1]\n        \n        #Record the best score\n        valid_score=model.best_score_['valid']['auc']\n        train_score=model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model,train_features, valid_features\n        gc.collect()\n        \n    submission=pd.DataFrame()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}